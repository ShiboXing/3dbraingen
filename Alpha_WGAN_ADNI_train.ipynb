{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from utils import *\n",
    "from ipdb import set_trace\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch import autograd\n",
    "# from torch.autograd import Variable\n",
    "import nibabel as nib\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import dataloader\n",
    "from nilearn import plotting\n",
    "from ADNI_dataset import *\n",
    "from BRATS_dataset import *\n",
    "from ATLAS_dataset import *\n",
    "from Model_alphaWGAN import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=4\n",
    "gpu = True\n",
    "workers = 4\n",
    "\n",
    "LAMBDA= 10\n",
    "_eps = 1e-15\n",
    "Use_BRATS=False\n",
    "Use_ATLAS = False\n",
    "\n",
    "#setting latent variable sizes\n",
    "latent_dim = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainset = ADNIdataset(augmentation=True, img_size=128)\n",
    "train_loader = torch.utils.data.DataLoader(trainset,batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True,num_workers=workers)\n",
    "# if Use_BRATS:\n",
    "#     #'flair' or 't2' or 't1ce'\n",
    "#     trainset = BRATSdataset(imgtype='flair')\n",
    "#     train_loader = torch.utils.data.DataLoader(trainset,batch_size = BATCH_SIZE, shuffle=True,\n",
    "#                                                num_workers=workers)\n",
    "# if Use_ATLAS:\n",
    "#     trainset = ATLASdataset(augmentation=True)\n",
    "#     train_loader = torch.utils.data.DataLoader(trainset,batch_size=BATCH_SIZE,\n",
    "#                                           shuffle=True,num_workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inf_train_gen(data_loader):\n",
    "    while True:\n",
    "        for _,images in enumerate(data_loader):\n",
    "            yield images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (conv1): Conv3d(1, 64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "  (conv2): Conv3d(64, 128, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "  (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv3d(128, 256, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "  (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv3d(256, 512, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "  (bn4): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv3d(512, 1000, kernel_size=(4, 4, 4), stride=(1, 1, 1))\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = Generator(noise = latent_dim)\n",
    "CD = Code_Discriminator(code_size = latent_dim ,num_units = 4096)\n",
    "D = Discriminator(is_dis=True)\n",
    "E = Discriminator(out_class = latent_dim,is_dis=False)\n",
    "\n",
    "G.cuda()\n",
    "D.cuda()\n",
    "CD.cuda()\n",
    "E.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_optimizer = optim.Adam(G.parameters(), lr=0.0002)\n",
    "d_optimizer = optim.Adam(D.parameters(), lr=0.0002)\n",
    "e_optimizer = optim.Adam(E.parameters(), lr = 0.0002)\n",
    "cd_optimizer = optim.Adam(CD.parameters(), lr = 0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradient_penalty(model, x, x_gen, w=10):\n",
    "    \"\"\"WGAN-GP gradient penalty\"\"\"\n",
    "    assert x.size()==x_gen.size(), \"real and sampled sizes do not match\"\n",
    "    alpha_size = tuple((len(x), *(1,)*(x.dim()-1)))\n",
    "    alpha_t = torch.cuda.FloatTensor if x.is_cuda else torch.Tensor\n",
    "    alpha = alpha_t(*alpha_size).uniform_()\n",
    "    #x_hat = x.data*alpha + x_gen.data*(1-alpha)\n",
    "    x_hat = x*alpha + x_gen*(1-alpha)\n",
    "    # x_hat = Variable(x_hat, requires_grad=True)\n",
    "    x_hat.requires_grad = True\n",
    "\n",
    "    def eps_norm(x):\n",
    "        x = x.view(len(x), -1)\n",
    "        return (x*x+_eps).sum(-1).sqrt()\n",
    "    def bi_penalty(x):\n",
    "        return (x-1)**2\n",
    "\n",
    "    grad_xhat = torch.autograd.grad(model(x_hat).sum(), x_hat, create_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    penalty = w*bi_penalty(eps_norm(grad_xhat)).mean()\n",
    "    return penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "#remove Variable usage\n",
    "real_y = torch.ones((BATCH_SIZE, 1)).cuda()#async=True))\n",
    "fake_y = torch.zeros((BATCH_SIZE, 1)).cuda()#async=True))\n",
    "\n",
    "criterion_bce = nn.BCELoss()\n",
    "criterion_l1 = nn.L1Loss()\n",
    "criterion_mse = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the highest savepoints of all models\n",
    "iteration = load_checkpoint(G, D, E, CD, '_iter')\n",
    "df = load_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 500.00 MiB (GPU 0; 11.00 GiB total capacity; 8.21 GiB already allocated; 143.74 MiB free; 8.23 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-03aabdd8193d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mz_rand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mz_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mx_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mx_rand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_rand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mc_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mCD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\---\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\3dbraingen\\Model_alphaWGAN.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, noise)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscale_factor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtp_conv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\---\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36minterpolate\u001b[1;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor)\u001b[0m\n\u001b[0;32m   3143\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupsample_nearest2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msfl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msfl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3144\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m5\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nearest'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3145\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupsample_nearest3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msfl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msfl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msfl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3146\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'area'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3147\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0madaptive_avg_pool1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 500.00 MiB (GPU 0; 11.00 GiB total capacity; 8.21 GiB already allocated; 143.74 MiB free; 8.23 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "g_iter = 1\n",
    "d_iter = 1\n",
    "cd_iter =1\n",
    "TOTAL_ITER = 12500\n",
    "gen_load = inf_train_gen(train_loader)\n",
    "arr = range(0, 64, 4)\n",
    "\n",
    "\n",
    "while iteration < TOTAL_ITER:\n",
    "    for p in D.parameters():  \n",
    "        p.requires_grad = False\n",
    "    for p in CD.parameters():  \n",
    "        p.requires_grad = False\n",
    "    for p in E.parameters():  \n",
    "        p.requires_grad = True\n",
    "    for p in G.parameters():  \n",
    "        p.requires_grad = True\n",
    "\n",
    "    ###############################################\n",
    "    # Train Encoder - Generator \n",
    "    ###############################################\n",
    "    for iters in range(g_iter):\n",
    "        G.zero_grad()\n",
    "        E.zero_grad()\n",
    "        real_images = gen_load.__next__()\n",
    "        _batch_size = real_images.size(0)\n",
    "        #remove Volatile Variable Usage\n",
    "        real_images = real_images.cuda()#async=True)\n",
    "        z_rand = torch.randn((_batch_size,latent_dim)).cuda()\n",
    "        z_hat = E(real_images).view(_batch_size,-1)\n",
    "        x_hat = G(z_hat)\n",
    "        x_rand = G(z_rand)\n",
    "        c_loss = -CD(z_hat).mean()\n",
    "        \n",
    "\n",
    "        d_real_loss = D(x_hat).mean()\n",
    "        d_fake_loss = D(x_rand).mean()\n",
    "        d_loss = -d_fake_loss-d_real_loss\n",
    "        l1_loss =10* criterion_l1(x_hat,real_images)\n",
    "        loss1 = l1_loss + c_loss + d_loss\n",
    "\n",
    "        if iters<g_iter-1:\n",
    "            loss1.backward()\n",
    "        else:\n",
    "            loss1.backward(retain_graph=True)\n",
    "        ## assign a copy of c_loss, remove it from the computational graph\n",
    "        c_loss = c_loss.clone().detach().requires_grad_(True)\n",
    "        e_optimizer.step() ## contains in-place operation to c_loss, leading to in-place op to loss3\n",
    "        g_optimizer.step()\n",
    "        g_optimizer.step()\n",
    "        set_trace()\n",
    "    ###############################################\n",
    "    # Train D\n",
    "    ###############################################\n",
    "    for p in D.parameters():  \n",
    "        p.requires_grad = True\n",
    "    for p in CD.parameters():  \n",
    "        p.requires_grad = False\n",
    "    for p in E.parameters():  \n",
    "        p.requires_grad = False\n",
    "    for p in G.parameters():  \n",
    "        p.requires_grad = False\n",
    "\n",
    "    for iters in range(d_iter):\n",
    "        d_optimizer.zero_grad()\n",
    "        real_images = gen_load.__next__()\n",
    "        _batch_size = real_images.size(0)\n",
    "        #remove Volatile Variable\n",
    "        z_rand = torch.randn((_batch_size,latent_dim)).cuda()\n",
    "        real_images = real_images.cuda()#async=True)\n",
    "        z_hat = E(real_images).view(_batch_size,-1)\n",
    "        x_hat = G(z_hat)\n",
    "        x_rand = G(z_rand)\n",
    "        x_loss2 = -2*D(real_images).mean()+D(x_hat).mean()+D(x_rand).mean()\n",
    "#         gradient_penalty_r = calc_gradient_penalty(D,real_images.data, x_rand.data)\n",
    "#         gradient_penalty_h = calc_gradient_penalty(D,real_images.data, x_hat.data)\n",
    "        gradient_penalty_r = calc_gradient_penalty(D,real_images, x_rand)\n",
    "        gradient_penalty_h = calc_gradient_penalty(D,real_images, x_hat)\n",
    "\n",
    "        loss2 = x_loss2+gradient_penalty_r+gradient_penalty_h\n",
    "        loss2.backward(retain_graph=True)\n",
    "        d_optimizer.step()\n",
    "\n",
    "    ###############################################\n",
    "    # Train CD\n",
    "    ###############################################\n",
    "    for p in D.parameters():  \n",
    "        p.requires_grad = False\n",
    "    for p in CD.parameters():  \n",
    "        p.requires_grad = True\n",
    "    for p in E.parameters():  \n",
    "        p.requires_grad = False\n",
    "    for p in G.parameters():  \n",
    "        p.requires_grad = False\n",
    "        \n",
    "    for iters in range(cd_iter):\n",
    "        cd_optimizer.zero_grad()\n",
    "        #remove volatile Variable usage\n",
    "        z_rand = torch.randn((_batch_size,latent_dim)).cuda()\n",
    "        z_rand.requires_grad = False\n",
    "#       gradient_penalty_cd = calc_gradient_penalty(CD,z_hat.data, z_rand.data)\n",
    "        gradient_penalty_cd = calc_gradient_penalty(CD,z_hat, z_rand)\n",
    "        loss3 = -CD(z_rand).mean() - c_loss + gradient_penalty_cd\n",
    "        loss3.backward(retain_graph=True)\n",
    "        cd_optimizer.step()\n",
    "\n",
    "    ###############################################\n",
    "    # Visualization\n",
    "    ###############################################\n",
    "\n",
    "    if iteration % 500 == 0:\n",
    "        lossStr = '[{}/{}]'.format(iteration,TOTAL_ITER) + '\\n\\\n",
    "        D: {:<8.3}'.format(loss2.item()) + '\\n\\\n",
    "        En_Ge: {:<8.3}'.format(loss1.item()) + '\\n\\\n",
    "        Code: {:<8.3}'.format(loss3.item()) \n",
    "        \n",
    "        print('lossStr', lossStr)\n",
    "        feat = np.squeeze((0.5*real_images[0]+0.5).cpu().numpy())\n",
    "        feat = nib.Nifti1Image(feat,affine = np.eye(4))\n",
    "        plotting.plot_img(feat,title=\"X_Real\")\n",
    "        plotting.show()\n",
    "\n",
    "        feat = np.squeeze((0.5*x_hat[0]+0.5).cpu().numpy())\n",
    "        feat = nib.Nifti1Image(feat,affine = np.eye(4))\n",
    "        plotting.plot_img(feat,title=\"DEC\")\n",
    "        \n",
    "        feat = np.squeeze((0.5*x_rand[0]+0.5).data.cpu().numpy())\n",
    "        feat = nib.Nifti1Image(feat,affine = np.eye(4))\n",
    "        plotting.plot_img(feat,title=\"x_rand\")\n",
    "        plotting.show()\n",
    "\n",
    "    ###############################################\n",
    "    # Model Save\n",
    "    ###############################################\n",
    "    if iteration % 500 == 0:\n",
    "        torch.save(G.state_dict(),'./checkpoint/G_iter'+str(iteration+1)+'.pth')\n",
    "        torch.save(D.state_dict(),'./checkpoint/D_iter'+str(iteration+1)+'.pth')\n",
    "        torch.save(E.state_dict(),'./checkpoint/E_iter'+str(iteration+1)+'.pth')\n",
    "        torch.save(CD.state_dict(),'./checkpoint/CD_iter'+str(iteration+1)+'.pth')\n",
    "        df = add_loss(df, iteration, loss1.item())\n",
    "        write_loss(df)\n",
    "    iteration += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
